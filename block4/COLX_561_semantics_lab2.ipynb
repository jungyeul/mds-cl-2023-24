{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 561 Lab Assignment 2: Predicates (Cheat sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will:\n",
    "- Write formulas in first-order logic corresponding to the semantics of simple sentences\n",
    "- Access semantic information contained in DBpedia and identify new semantic relationships "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "For this assignment, you will need the \"Mapping Based Objects\" from the 2016 version of DBpedia, which can be downloaded directly from the DBpedia site [here](http://downloads.dbpedia.org/2016-10/core-i18n/en/mappingbased_objects_en.ttl.bz2). This file is too large to be included in your Github repo, so you should ignore the lab instructions and store it elsewhere. Do **NOT** unzip the file, keep it as is.\n",
    "\n",
    "Run the code below to access relevant modules (you can add to this as needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "# !python3 -m pip install --user rdflib\n",
    "# !python3 -m pip install --user owlrl\n",
    "\n",
    "import nltk\n",
    "read_expr = nltk.sem.Expression.fromstring\n",
    "\n",
    "import bz2\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.sem import Valuation, Model,Assignment\n",
    "\n",
    "import rdflib\n",
    "from rdflib import URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, FOAF, OWL\n",
    "import owlrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)\n",
    "- Except that you should **NOT** include the data in your repo, put it elsewhere on your hard drive and modify the path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "path_to_data = \"/Users/your-path-to/\" # change this path!\n",
    "\n",
    "# wget https://downloads.dbpedia.org/2016-10/core-i18n/en/mappingbased_objects_en.ttl.bz2\n",
    "# MDS-CL % du -h mappingbased_objects_en.ttl.bz2 \n",
    "# 176M\tmappingbased_objects_en.ttl.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: First order logic for sentences\n",
    "\n",
    "For each of the sentences below, write an expression in first order logic which represents the semantics of the sentence. Assign it a variable `pn` (e.g. `p0`, `p1`, `p2`, etc.) where `n` is the subexercise number. An example is given for you (which you should use as a premise for **Exercise 1.7**).\n",
    "\n",
    "*John is a person*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "p0 = read_expr(\"person(John)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples:\n",
    "\n",
    "- _every undergraduate student is a student_: $\\forall_x.(\\text{undergraduate\\_student}(x) \\rightarrow \\text{student}(x))$\n",
    "- _all cubes are small_: $\\forall_x.(\\text{cube}(x) \\rightarrow \\text{small}(x))$\n",
    "- _Every player for Tottenham Hotspur F.C. is a good player_: $\\forall_x.(\\text{play\\_for}(x, \\text{Tottenham}) \\rightarrow \\text{good}(x))$\n",
    "- _All blue cars are fast_: $\\forall_x.(\\text{blue}(x) \\land \\text{car}(x) \\rightarrow \\text{fast}(x))$\n",
    "- _John owns a house_: $\\exists_x.(\\text{owns}(\\text{John}, x) \\land \\text{house}(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1\n",
    "\n",
    "rubric={raw:1}\n",
    "\n",
    "*Fluffy is a black cat and Rover is a black dog*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2\n",
    "rubric={raw:1}\n",
    "\n",
    "*Dogs and cats are both kinds of pets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3\n",
    "\n",
    "rubric={raw:1}\n",
    "\n",
    "*People like pets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.4\n",
    "\n",
    "rubric={raw:2}\n",
    "\n",
    "*Black cats don't like black dogs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.5\n",
    "\n",
    "rubric={raw:2}\n",
    "\n",
    "*Cats are happy as long as someone likes them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.6\n",
    "\n",
    "rubric={raw:2}\n",
    "\n",
    "*Dogs aren't happy if there is anyone (or anything) that dislikes them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p6 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.7\n",
    "\n",
    "rubric={raw:2}\n",
    "\n",
    "Convert each of the two sentences below into first order logic and then prove they follow from (are implied by) the sentences above. In each case you should **only** include relevant premises (if you include irrelevant premises your code may crash!).\n",
    "\n",
    "*Fluffy is happy*\n",
    "\n",
    "*Rover is not happy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = read_expr(\"happy(Fluffy)\")\n",
    "nltk.TableauProver().prove(c, [which logic ... p0,p1,p2,p3,p4,p5,p6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = read_expr(\"-happy(Rover)\")\n",
    "nltk.TableauProver().prove(c, [which logic ...])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Accessing the RDF triples in DBpedia\n",
    "\n",
    "rubric={accuracy:2}\n",
    "\n",
    "For **Exercises 3-5**, you will be using RDF triples derived from the infoboxes of English Wikipedia. You need to load them and convert them into a readable format. Some starter code is provided, which loads the file assuming it is the `path_to_data` directory (still compressed in bz2 format) and iterates line by line. The lines are formatted as follows:\n",
    "\n",
    "```\n",
    "<http://dbpedia.org/resource/Alabama> <http://dbpedia.org/ontology/country> <http://dbpedia.org/resource/United_States> .\n",
    "```\n",
    "\n",
    "We will refer to the first of these triples as the *subject*, the second as the *predicate*, and the third as the *object*. We are only interested in triples where there is a `dbpedia.org` URL for all three elements of the triple! Otherwise, you should remove the nondistinctive parts of the URL from each element and add the triple to a Python dict which maps from a `(subject, object)` tuple to a list of all the predicates the pair participates in. (This format is less efficient than storing mappings from predicates to `(subjet, object)` pairs, but very useful for Exercise 3)\n",
    "\n",
    "For example, if you only read in the line above, you would create a dictionary like this:\n",
    "\n",
    "```\n",
    "predicates = {(\"Alabama\", \"United States\"): [\"country\"]}\n",
    "```\n",
    "\n",
    "At the same time, create a mapping from each of your identifiers (e.g. \"Alabama\", \"United States\", and \"country\" in the example above)  to its original URI (e.g. ```http://dbpedia.org/resource/Alabama```) in a Python dict called `uri_lookup`, you'll use this in exercise 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "MDS-CL % gzcat mappingbased_objects_en.ttl.bz2 | head\n",
    "# started 2017-05-25T05:44:10Z\n",
    "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/2000/01/rdf-schema#seeAlso><http://dbpedia.org/resource/Anarchist_terminology> . <-- KO\n",
    "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/2000/01/rdf-schema#seeAlso> <http://dbpedia.org/resource/Anarchism> . <-- KO\n",
    "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/2000/01/rdf-schema#seeAlso> <http://dbpedia.org/resource/France> . <-- KO\n",
    "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/2000/01/rdf-schema#seeAlso> <http://dbpedia.org/resource/Violence> . <-- KO\n",
    "<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/2000/01/rdf-schema#seeAlso> <http://dbpedia.org/resource/Education> . <-- KO\n",
    "<http://dbpedia.org/resource/Alabama> <http://dbpedia.org/ontology/country> <http://dbpedia.org/resource/United_States> . <-- OK\n",
    "<http://dbpedia.org/resource/Alabama> <http://dbpedia.org/ontology/language> <http://dbpedia.org/resource/English_American> . <-- OK\n",
    "<http://dbpedia.org/resource/Alabama> <http://dbpedia.org/ontology/capital> <http://dbpedia.org/resource/Montgomery,_Alabama> . <-- OK\n",
    "<http://dbpedia.org/resource/Alabama> <http://dbpedia.org/ontology/largestCity> <http://dbpedia.org/resource/Birmingham,_Alabama> . <-- OK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infobox: \n",
    "\n",
    "![Alabama](alabama.jpg)\n",
    "\n",
    "\n",
    "- **Semantic Web** is to make Internet data machine-readable (source: https://en.wikipedia.org/wiki/Semantic_Web)\n",
    "- **Linked Data**  is structured data which is interlinked with other data so it becomes more useful through semantic queries (source: https://en.wikipedia.org/wiki/Linked_data)\n",
    "- **SPARQL** is an RDF query language—that is, a semantic query language for databases—able to retrieve and manipulate data stored in Resource Description Framework (RDF) format (source: https://en.wikipedia.org/wiki/SPARQL)\n",
    "- **Entity linking**, also referred to as named-entity linking (NEL), named-entity disambiguation (NED), named-entity recognition and disambiguation (NERD) or named-entity normalization (NEN), is the task of assigning a unique identity to entities (such as famous individuals, locations, or companies) mentioned in text.\n",
    "\n",
    "\n",
    "\n",
    "![Paris](paris.png)\n",
    "\n",
    "\n",
    "\n",
    "- *Paris* (mythology), a prince of Troy in Greek mythology\n",
    "- *Paris* (city) is the capital of France.\n",
    "- *Paris, Texas* (film), a 1984 drama directed by Wim Wenders\n",
    "- *Paris* Hilton (person), is an American media personality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **DBpedia** is a project aiming to extract structured content from the information created in the Wikipedia project (source: https://en.wikipedia.org/wiki/DBpedia / https://wiki.dbpedia.org)\n",
    "\n",
    "- **Freebase**  was a large collaborative knowledge base consisting of data composed mainly by its community members (source: https://en.wikipedia.org/wiki/Freebase_(database) / https://developers.google.com/freebase)\n",
    "\n",
    "- **YAGO (Yet Another Great Ontology)** is an open source knowledge base developed at the Max Planck Institute for Computer Science in Saarbrücken. It is automatically extracted from Wikipedia and other sources (source: https://en.wikipedia.org/wiki/YAGO_(database) / https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago)\n",
    "\n",
    "- SemEval-2021 Task $>$ Information in scientific \\& clinical text $>$ Task 11: **NLPContributionGraph** by Jennifer D’Souza, Sören Auer, and Ted Pedersen (https://ncg-task.github.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes long....\n",
    "\n",
    "#my code here\n",
    "def extract_name(url):\n",
    "    return url[url.rfind(\"/\")+1:-1].replace(\"_\", \" \")\n",
    "#my code here\n",
    "\n",
    "predicates = defaultdict(list)\n",
    "uri_lookup = {}\n",
    "f = bz2.open(path_to_data + \"mappingbased_objects_en.ttl.bz2\", \"rt\", encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    # your code here\n",
    "\n",
    "    # your code here\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert len(predicates) == 17320526\n",
    "assert ('Animalia (book)', 'Graeme Base') in predicates\n",
    "assert predicates[('Animalia (book)', 'Graeme Base')] == ['author', 'illustrator']\n",
    "assert uri_lookup[\"Animalia (book)\"] == \"http://dbpedia.org/resource/Animalia_(book)\"\n",
    "assert len(uri_lookup) == 5381326\n",
    "print(\"Success!\")\n",
    "\n",
    "# # assert len(predicates_pickle) == 17320526\n",
    "# # assert ('Animalia (book)', 'Graeme Base') in predicates_pickle\n",
    "# # assert predicates_pickle[('Animalia (book)', 'Graeme Base')] == ['author', 'illustrator']\n",
    "# # assert url_lookup_pickle[\"Animalia (book)\"] == \"http://dbpedia.org/resource/Animalia_(book)\"\n",
    "# # assert len(url_lookup_pickle) == 5381326\n",
    "# # print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implied predicates\n",
    "\n",
    "As with *pet* and *dog*, one predicate will often imply the other. Your first task using the DBpedia data is to identify such predicates. In a bottom-up approach, we can identify such predicates by noting that, if Pred1 implies Pred2, any `(subject, object)` argument pair (from our `predicates` list built above) which has Pred1 in its list of predicates will also have Pred2, which in turn means that the count of argument pairs with have both Pred1 and Pred2 as valid predicates will be equal to the number with just Pred1.\n",
    "\n",
    "#### Exercise 3.1\n",
    "\n",
    "rubric={accuracy:2,efficiency:1}\n",
    "\n",
    "The first thing you need to do to derive your implication is to complete the `get_pred_counts` which will count how often individual predicates appear in `predicates`, and how often pairs of predicates appear together (with the same arguments). You should avoid creating duplicate counts for pairs in opposite order (i.e. you should have `(\"officialLanguage\",\"language\")` or `(\"language\",\"officialLanguage\")`, but not both!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pred_counts[\"officialLanguage\"] = 561`\n",
    "\n",
    "`pred_counts[\"language\"] = 108030`\n",
    "\n",
    "\n",
    "```\n",
    "('Canada', 'O_Canada')\tanthem\n",
    "('Canada', 'God_Save_the_Queen')\tanthem\n",
    "('Canada', 'Federalism')\tgovernmentType\n",
    "('Canada', 'Parliamentary_system')\tgovernmentType\n",
    "('Canada', 'Representative_democracy')\tgovernmentType\n",
    "('Canada', 'Constitutional_monarchy')\tgovernmentType\n",
    "('Canada', 'Canadian_dollar')\tcurrency\n",
    "('Canada', 'Ottawa')\tcapital\n",
    "('Canada', 'Toronto')\tlargestCity\n",
    "('Canada', 'English_language') officialLanguage         <--\n",
    "('Canada', 'French_language') officialLanguage         \n",
    "('Canada', 'English_language') language                 <--\n",
    "('Canada', 'French_language')  language                 \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pred_pair_counts[(\"language\",\"officialLanguage\")] = 561` where `language` > `officialLanguage`\n",
    "\n",
    "```\n",
    "('Canada', 'English_language') officialLanguage     <---|\n",
    "('Canada', 'French_language') officialLanguage          |\n",
    "('Canada', 'English_language') language             <---|\n",
    "('Canada', 'French_language') language\n",
    "('Canada', 'Cree_language') language\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_counts(predicates):\n",
    "    '''This function counts how many times predicates appear individually (pred_counts) and \n",
    "    how often they appear together with the same (subject, object) pair (pred_pair_counts)\n",
    "    It returns a tuple with each of these counts, which are Counters'''\n",
    "    pred_counts = Counter()\n",
    "    pred_pair_counts = Counter()\n",
    "    # your code here\n",
    " \n",
    "    # your code here\n",
    "    return pred_counts,pred_pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language > officialLanguage\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "pred_counts, pred_pair_counts = get_pred_counts(predicates)\n",
    "assert pred_counts[\"officialLanguage\"] == 561\n",
    "assert (\"officialLanguage\",\"language\") in pred_pair_counts or (\"language\",\"officialLanguage\") in pred_pair_counts\n",
    "if (\"officialLanguage\",\"language\") in pred_pair_counts:\n",
    "    assert (\"language\",\"officialLanguage\") not in pred_pair_counts\n",
    "    assert pred_pair_counts[(\"officialLanguage\",\"language\")] == 561\n",
    "else:\n",
    "    print(\"language > officialLanguage\")\n",
    "    assert pred_pair_counts[(\"language\",\"officialLanguage\")] == 561    \n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n",
    "\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Now use the output of `get_pred_counts` to identify predicates which imply one another, based on the logic discussed above. Print out a list of the predicate pairs you find in the form of a logical implicature (i.e. `predicate_1 -> predictate_2`). Some predicates are equivalent based on the data (the implication goes both ways), for those you should print out both implications (`predicate_1 -> predictate_2` and `predicate_2 -> predictate_1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`officialLanguage -> language` where `pred_counts['officialLanguage'] = 561` $<$ `pred_counts['language'] = 108030`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officialLanguage -> language\n",
      "associatedMusicalArtist -> associatedBand\n",
      "mouthMountain -> mouthPlace\n",
      "mouthPlace -> mouthMountain\n",
      "stateOfOrigin -> nationality\n",
      "sourceMountain -> sourcePlace\n",
      "sourcePlace -> sourceMountain\n",
      "capitalMountain -> capitalPlace\n",
      "capitalPlace -> capitalMountain\n",
      "regionalLanguage -> language\n",
      "sourceConfluenceMountain -> sourceConfluencePlace\n",
      "sourceConfluencePlace -> sourceConfluenceMountain\n",
      "lowestMountain -> lowestPlace\n",
      "capitalCountry -> state\n",
      "highestMountain -> highestPlace\n",
      "distributingCompany -> distributingLabel\n",
      "distributingLabel -> distributingCompany\n",
      "governmentMountain -> governmentPlace\n",
      "governmentPlace -> governmentMountain\n",
      "managementMountain -> managementPlace\n",
      "managementPlace -> managementMountain\n",
      "legalForm -> type\n",
      "firstLaunchRocket -> associatedRocket\n",
      "lastLaunchRocket -> associatedRocket\n",
      "firstLaunchRocket -> lastLaunchRocket\n",
      "lastLaunchRocket -> firstLaunchRocket\n",
      "projectCoordinator -> projectParticipant\n"
     ]
    }
   ],
   "source": [
    "pred_counts, pred_pair_counts = get_pred_counts(predicates)\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3\n",
    "\n",
    "rubric={accuracy:3,efficiency:1}\n",
    "\n",
    "Pick one of the pairs of predicates you found above, and (programmatically) build an NLTK semantic model of the data you have for those two predicates (and just those two predications). The code from lecture for doing that is provided, but you must create a representation that can be turned into a valuation (call it `v`). Then prove (using the `m.evaluate()` method) that one implies the other in the model you've built Hint: your final logical formula should involve universal quantification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> import nltk\n",
    ">>> from nltk.sem import Valuation, Model\n",
    ">>> v = [('adam', 'b1'), ('betty', 'g1'), ('fido', 'd1'),\n",
    "... ('girl', set(['g1', 'g2'])), ('boy', set(['b1', 'b2'])),\n",
    "... ('dog', set(['d1'])),\n",
    "... ('love', set([('b1', 'g1'), ('b2', 'g2'), ('g1', 'b1'), ('g2', 'b1')]))]\n",
    ">>> val = Valuation(v)\n",
    ">>> dom = val.domain\n",
    ">>> m = Model(dom, val)\n",
    ">>> g = nltk.sem.Assignment(dom)\n",
    ">>> m.evaluate('all x.(boy(x) -> - girl(x))', g)\n",
    "True\n",
    "```\n",
    "\n",
    "See http://www.nltk.org/howto/semantics.html (nltk Semantics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_preds = {\"language\": set(), \"officialLanguage\": set()}\n",
    "# your code here\n",
    "\n",
    "...\n",
    "# your code here\n",
    "\n",
    "val = Valuation(v)\n",
    "dom = val.domain\n",
    "m = Model(dom, val)\n",
    "g = Assignment(dom)\n",
    "\n",
    "# your code here\n",
    "m.evaluate('all x.(boy(x) -> - girl(x))', g) #  officialLanguage -> language\n",
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(wanted_preds['language'])`:\n",
    "```\n",
    "{('He Died Fifteen Years Ago', 'Spanish language'),\n",
    " ('Achha Bura', 'Standard Hindi'),\n",
    " ('Murut people', 'Malaysian language'),\n",
    " ('The Hunt (1963 film)', 'Cinema of Portugal'),\n",
    " (\"The Captain's Ship\", 'Spanish language'),\n",
    " ('Khmer people', 'Khmer language'),\n",
    " ('Granma (newspaper)', 'Spanish language'),\n",
    "... }\n",
    "```\n",
    "\n",
    "`print(wanted_preds['officialLanguage'])`:\n",
    "```\n",
    "{('Abkhazia', 'Abkhaz language'),\n",
    " ('Abkhazia', 'Russian language'),\n",
    " ('Abyei', 'Arabic language'),\n",
    " ('Adjara', 'Georgian language'),\n",
    " ('Adélie Land', 'French language'),\n",
    "...}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Adding a new DBpedia predictate\n",
    "\n",
    "DBpedia has two predicates related to making the connection between authors and their works: `author` which has the book as the subject and author as the object (it is derived from the field in the info box of the wikipedia page associated with a book) and `notableWork` which has the author as the subject and the work as the object (it is derived the wikipedia page associated with the author). The `notableWork` predicate, however, only mentions the author's most notable work. You're going to create a new predicate `work` which is like `notableWork` in argument structure but includes the information from both `notableWork` and `Author` predicate (if Y is a `notableWork` of X, then Y is also a `work` of X; if  X is an `author` of Y, then Y is a `work` of X), and then place class restrictions on the subject and object of your new prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `(Haruki Murakami, notableWork, 1Q84)` $\\rightarrow$ `(Haruki Murakami, work, 1Q84)`\n",
    "- `(1Q84, author, Haruki Murakami)` $\\rightarrow$ `(Haruki Murakami, work, 1Q84)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 \n",
    "rubric={accuracy:2}\n",
    "\n",
    "First, create an rdflib graph which contains the information associated with the `notableWork` and `author` predicates in DBpedia (and only that information). You should use the Python data structures you built in exercise 2 (`predicates` and `uri_lookup`), rather than loading the rdf from disk. Since later parts of this exercise can take a lot of time with the full dataset, it's a good idea to just load in a part of the data now, when you're setting things up/debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Graph identifier=N0badbf399d6a47c0bd95958eb10315f4 (<class 'rdflib.graph.Graph'>)>\n"
     ]
    }
   ],
   "source": [
    "g = rdflib.Graph()\n",
    "\n",
    "# your code here\n",
    "\n",
    "# add \"notableWork\" in pred_list to g with its sbj and obj; \n",
    "# g should contain: \n",
    "#   URIRef(\"http://dbpedia.org/resource/Graeme_Base\"),      <- sbj\n",
    "#   URIRef(\"http://dbpedia.org/ontology/notableWork\"),      <- predicate\n",
    "#   URIRef(\"http://dbpedia.org/resource/Animalia_(book)\")   <- obj\n",
    "\n",
    "# add \"author\" in pred_list to g with its sbj and obj\n",
    "    \n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert len(g) == 68295 # 36228\n",
    "assert (URIRef(\"http://dbpedia.org/resource/Animalia_(book)\"), URIRef(\"http://dbpedia.org/ontology/author\"), URIRef(\"http://dbpedia.org/resource/Graeme_Base\")) in g\n",
    "assert (URIRef(\"http://dbpedia.org/resource/Graeme_Base\"), URIRef(\"http://dbpedia.org/ontology/notableWork\"), URIRef(\"http://dbpedia.org/resource/Animalia_(book)\")) in g\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2\n",
    "rubric={accuracy:2}\n",
    "\n",
    "**Define a `work` predicate** (pretend it is an existing DBpredia predicate) and then **add two additional edges to the graph which indicate the OWL relationship to `author` ('inverse of') and `notableWork` ('subproperty of')**. Then deduce the individual relationships for `work` using `owlrl`. This takes a while for the full dataset. You will get no points on this problem if you manually add the edges corresponding to individual instances of the `work` predicate to the graph, you must deduce them from the other predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes long......\n",
    "\n",
    "notable_work_ref = URIRef(uri_lookup[\"notableWork\"])\n",
    "author_ref = URIRef(uri_lookup[\"author\"])\n",
    "work_ref = URIRef(\"http://dbpedia.org/ontology/work\")\n",
    "\n",
    "# your code here\n",
    "# add 'work' is insverse of 'author'\n",
    "# add 'notable work' is subproperty of 'work'\n",
    "\n",
    "# your code here\n",
    "\n",
    "owlrl.DeductiveClosure(owlrl.CombinedClosure.RDFS_OWLRL_Semantics).expand(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for subj,pred,obj in g:\n",
    "    if \"work\" in pred:\n",
    "        count += 1\n",
    "assert count == 65563 # 34683\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3\n",
    "rubric={accuracy:2, quality:1}\n",
    "\n",
    "Add appropriate class restrictions on subjects and predicates of your new `work` predicate, i.e. that the subject much be a person and the object must be a book. **Recall that `RDFS.domain` is used to limit the subject ('person'), and `RDFS.range` the object ('book')**. You can just make up URIs for your classes, like we did in lecture. Again, do not assign your classes to particular authors/books manually, you must deduce them!\n",
    "\n",
    "After you have deduced the classes, create a test which shows that all the books from the original `author` relation have been identified as books (assigned your new book class) via the restrictions on the `work` predicate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes long..\n",
    "\n",
    "book_ref = URIRef(\"http://example.org/thing/book\")\n",
    "person_ref = URIRef(\"http://example.org/thing/person\")\n",
    "\n",
    "# your code here\n",
    "# add work as RDFS.domain to person\n",
    "# add work as RDFS.range to book\n",
    "\n",
    "# your code here\n",
    "\n",
    "owlrl.DeductiveClosure(owlrl.CombinedClosure.RDFS_OWLRL_Semantics).expand(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# tests here\n",
    "for sbj_obj_pair, pred_list in predicates.items():\n",
    "    sbj, obj = sbj_obj_pair\n",
    "    if \"author\" in pred_list:\n",
    "        assert (URIRef(uri_lookup[sbj]),RDF.type,book_ref) in g\n",
    "print (\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Properties (optional)\n",
    "rubric={accuracy:1,reasoning:1}\n",
    "\n",
    "There are no explict properties, in the sense of a one-argument predicates, in the DBpedia data (or the RDF framework generally); everything is expressed in terms of two-argument predicates. In this exercise, you will try to automatically identify potential properties contained in the data. Once you have identified some likely candidates, write about how you found them, whether you think your method always identifies good properties (is there something better you might do?), and provide names for a few distinct properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('type', 'Town')\n",
      "('family', 'Arctiidae')\n",
      "('deathPlace', 'United States')\n",
      "('family', 'Tortricidae')\n",
      "('occupation', 'Actress')\n",
      "('birthPlace', 'India')\n",
      "('family', 'Noctuidae')\n",
      "('genre', 'Soul music')\n",
      "('kingdom', 'Fungi')\n",
      "('genre', 'Heavy metal music')\n",
      "('genre', 'Contemporary R&B')\n",
      "('family', 'Geometridae')\n",
      "('genre', 'Punk rock')\n",
      "('position', 'Defender (football)')\n",
      "('genre', 'Folk music')\n",
      "('birthPlace', 'Italy')\n",
      "('position', 'Midfielder (association football)')\n",
      "('type', 'Census-designated place')\n",
      "('position', 'Striker (association football)')\n",
      "('birthPlace', 'France')\n",
      "('birthPlace', 'Japan')\n",
      "('type', 'City')\n",
      "('isPartOf', 'Masovian Voivodeship')\n",
      "('recordLabel', 'Columbia Records')\n",
      "('position', 'Pitcher')\n",
      "('genre', 'Pop rock')\n",
      "('nationality', 'United Kingdom')\n",
      "('country', 'Spain')\n",
      "('mediaType', 'Hardcover')\n",
      "('country', 'Turkey')\n",
      "('genre', 'Hard rock')\n",
      "('family', 'Crambidae')\n",
      "('genre', 'Indie rock')\n",
      "('position', 'Goalkeeper (association football)')\n",
      "('battle', 'World War II')\n",
      "('birthPlace', 'Canada')\n",
      "('occupation', 'Actor')\n",
      "('party', 'Republican Party (United States)')\n",
      "('country', 'Italy')\n",
      "('format', 'CD single')\n",
      "('class', 'Monocots')\n",
      "('location', 'United States')\n",
      "('genre', 'Hip hop music')\n",
      "('country', 'Romania')\n",
      "('format', 'Music download')\n",
      "('timeZone', 'Eastern Time Zone')\n",
      "('order', 'Rosids')\n",
      "('country', 'England')\n",
      "('genre', 'Country music')\n",
      "('party', 'Democratic Party (United States)')\n",
      "('timeZone', 'North American Eastern Time Zone')\n",
      "('family', 'Cerambycidae')\n",
      "('order', 'Asterids')\n",
      "('type', 'Unincorporated area')\n",
      "('genre', 'Jazz')\n",
      "('timeZone', 'North American Central Time Zone')\n",
      "('class', 'Actinopterygii')\n",
      "('genre', 'Alternative rock')\n",
      "('country', 'Germany')\n",
      "('birthPlace', 'England')\n",
      "('phylum', 'Chordate')\n",
      "('order', 'Polyphaga')\n",
      "('country', 'Australia')\n",
      "('timeZone', 'Eastern European Summer Time')\n",
      "('genre', 'Rock music')\n",
      "('timeZone', 'Eastern European Time')\n",
      "('country', 'Canada')\n",
      "('type', 'Album')\n",
      "('class', 'Gastropoda')\n",
      "('kingdom', 'Plant')\n",
      "('position', 'Forward (association football)')\n",
      "('language', 'English language')\n",
      "('phylum', 'Mollusca')\n",
      "('birthPlace', 'United States')\n",
      "('timeZone', 'Indian Standard Time')\n",
      "('genre', 'Pop music')\n",
      "('nationality', 'United States')\n",
      "('order', 'Beetle')\n",
      "('timeZone', 'Central European Summer Time')\n",
      "('phylum', 'Chordata')\n",
      "('kingdom', 'Plantae')\n",
      "('kingdom', 'Animalia')\n",
      "('position', 'Defender (association football)')\n",
      "('timeZone', 'Central European Time')\n",
      "('class', 'Eudicots')\n",
      "('position', 'Midfielder')\n",
      "('country', 'France')\n",
      "('country', 'India')\n",
      "('division', 'Angiosperms')\n",
      "('country', 'United Kingdom')\n",
      "('type', 'Village')\n",
      "('country', 'Poland')\n",
      "('timeZone', 'Iran Daylight Time')\n",
      "('timeZone', 'Iran Standard Time')\n",
      "('country', 'Iran')\n",
      "('order', 'Lepidoptera')\n",
      "('country', 'United States')\n",
      "('class', 'Insect')\n",
      "('phylum', 'Arthropod')\n",
      "('kingdom', 'Animal')\n"
     ]
    }
   ],
   "source": [
    "pred_obj_count = {} # predicate-object pair, for example; \n",
    "\n",
    "# your code here\n",
    "\n",
    "# your code here\n",
    "# print some 100 examples..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
