{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *International Workshop on Semantic Evaluation*\n",
    "\n",
    "|   |   |   |\n",
    "|---|---|---|\n",
    "|Senseval-1\t1998 | Sussex || \n",
    "|Senseval-2\t2001| Toulouse@ACL ||\n",
    "|Senseval-3\t2004|  Barcelona ||\n",
    "|SemEval-2007| Prague@ACL ||\n",
    "|SemEval-2010|  Uppsala@ACL ||\n",
    "|SemEval-2012|  Montreal@NAACL ||\n",
    "|SemEval-2013| Atlanta@NAACL ||\n",
    "|... | ||\n",
    "|SemEval-2020| Barcelona@COLING |https://alt.qcri.org/semeval2020/index.php?id=tasks |\n",
    "|SemEval-2021| Bangkok@ACL-IJCNLP | https://semeval.github.io/SemEval2021/tasks |\n",
    "|SemEval-2022| Seattle@NAACL| https://semeval.github.io/SemEval2022/tasks |\n",
    "|SemEval-2023|| https://semeval.github.io/SemEval2023/|\n",
    "\n",
    "* https://semeval.github.io/SemEval2023/tasks.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 561 Lab Assignment 1: Word Sense Disambiguation Project (Cheat sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment objectives\n",
    "\n",
    "In this assignment you will:\n",
    "- Do automatic word sense disambiguation (WSD) using the context around an ambigious word\n",
    "- Apply the semantic knowledge in WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "This assignment requires that you have downloaded following NLTK corpora/lexicons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package senseval to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package senseval is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "nltk.download('senseval')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import senseval, stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "OPEN_CLASS_POS = {'n', 'v', 'j', 'r'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK version of the Senseval 2 formatted files from *Senseval-3* uses well-formed XML. Each instance of the ambiguous words *hard*, *interest*, *line*, and *serve* is tagged with a sense identifier, and supplied with context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:2}\n",
    "\n",
    "- You have been assigned a team for this project, which you will find in the teams.txt file on the COLX 561 course repo\n",
    "- One person in each group must create a private UBC github repo, and give access to all group members as well as the members of the teaching team\n",
    "- In the readme in the individual lab repo (the one created when the lab is opened) for all members of the group, you should have a link to this private, shared repo. Pushing that link is your only \"submission\". Don't put anything else in your repo for this lab.\n",
    "- In the readme of the private shared repo, include instructions that will allow someone to reproduce your results and identify the parts of the code relevant to the major sections discussed below. \n",
    "- You do NOT have to use the ipynb you are reading; You may find it useful to have multiple ipynbs (or even standalone .py files which your .ipynb(s) import) for different parts of the assignment, just make sure you indicate how your code should be run (You should assume it will be run). You can also just add cells to this ipynb, whatever works for you.\n",
    "- Note that any commits on the private shared repo after the deadline will result in a late penalty being applied to the project, so be careful about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Setting up the WSD task\n",
    "\n",
    "rubric={accuracy:2,quality:1}\n",
    "\n",
    "For this lab, you will be using the Senseval corpus included in NLTK, which has sense-tagged data for a small set of word types. In this lab, we will only look at the ambiguity of the word *line*. Note that this corpus is arranged in a way that is **NOT** typical for NLTK corpora. It is stored in a list of *instances*, where each instance has the sense and the context around it. You can iterate over instances of the word *line* using this code: `for instance in senseval.instances('line.pos')`. It is up to you to figure out how to extract the exact information you need from each instance (you don't need to store information other than the sense and context for each instance).  It might be worthwhile to do some debugging to see what these instances look like.\n",
    "\n",
    "Your first goal is to reorganize the information into two Python dictionaries: *train* and *test*.\n",
    "Each dictionary will contain senses as the keys, while the values are lists of POS-tagged sentences (if an *instance* in the semeval corpus has the given sense, it is included in this list).\n",
    "\n",
    "The first 200 instances will be stored in the *test* dictionary, while all the rest of the instances will be in the *train* dictionary.\n",
    "\n",
    "When you have solved the problem, please add test cases which confirm the below: \n",
    "\n",
    "1. both dictionaries contain information relating to 6 senses of line (ie, the dictionary has 6 keys), including a \"product\" sense\n",
    "2. for the \"product\" sense of *line*\n",
    "   2. the `test` dictionary has 200 context sentences.\n",
    "   1. the `train` dictionary contains 2017 context sentences.\n",
    "   2. the first context sentence of `train` has 49 tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The NLTK version of the Senseval 2 (*formatted*) files (from *Senseval-3*) uses well-formed XML.\n",
    "Each instance of the ambiguous words ***hard***, ***interest***, ***line***, and ***serve*** is tagged with a sense identifier, and supplied with context.\n",
    "\n",
    "\n",
    "source: \n",
    "\n",
    "* [nltk] https://www.nltk.org/_modules/nltk/corpus/reader/senseval.html\n",
    "* [senseval] https://www.d.umn.edu/~tpederse/data.html\n",
    "\n",
    "\n",
    "Gale, W. A., Church, K. W., & Yarowsky, D. (1992). One Sense Per Discourse. *Proceedings of the Workshop on Speech and Natural Language*, 233â€“237. https://doi.org/10.3115/1075527.1075579.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for instance in senseval.instances('line.pos'):\n",
    "    ...\n",
    "```\n",
    "\n",
    "```\n",
    "SensevalInstance(word='line-n', position=16, context=[('perhaps', 'RB'), ('not', 'RB'), ('surprisingly', 'RB'), (',', ','), ('the', 'DT'), ('locals', 'NNS'), ('often', 'RB'), ('call', 'VBP'), ('the', 'DT'), ('warden', 'NN'), ('.', '.'), ('while', 'IN'), ('noodlers', 'NNS'), ('generally', 'RB'), ('drag', 'VBP'), ('a', 'DT'), ('line', 'NN'), ('with', 'IN'), ('a', 'DT'), ('big', 'JJ'), ('hook', 'NN'), ('on', 'IN'), ('it', 'PRP'), ('through', 'IN'), ('the', 'DT'), ('water', 'NN'), ('trying', 'VBG'), ('to', 'TO'), ('snag', 'VB'), ('a', 'DT'), ('fish', 'NN'), (',', ','), ('mr', 'NNP'), ('.', '.'), ('willaby', 'NNP'), ('wades', 'NNS'), ('along', 'IN'), ('river', 'NN'), ('banks', 'NNS'), ('and', 'CC'), ('lake', 'NN'), ('shores', 'NNS'), ('until', 'IN'), ('he', 'PRP'), ('finds', 'VBZ'), ('holes', 'NNS'), ('where', 'WRB'), ('fat', 'JJ'), ('catfish', 'NN'), ('are', 'VBP'), ('laying', 'VBG'), ('eggs', 'NNS'), ('.', '.'), ('he', 'PRP'), ('dives', 'VBZ'), ('down', 'RB'), ('and', 'CC'), ('pokes', 'VBZ'), ('his', 'PRP$'), ('rod', 'NN'), ('and', 'CC'), ('a', 'DT'), ('few', 'JJ'), ('inches', 'NNS'), ('of', 'IN'), ('line', 'NN'), ('with', 'IN'), ('a', 'DT'), ('baited', 'VBN'), ('hook', 'NN'), ('into', 'IN'), ('the', 'DT'), ('nest', 'NN'), ('until', 'IN'), ('the', 'DT'), ('fish', 'NN'), ('bites', 'NNS'), ('.', '.')], senses=('cord',))\n",
    "```\n",
    "\n",
    "```\n",
    "instance.senses[0] = {cord | division | formation | phone | product | text}\n",
    "\n",
    "                      test : train\n",
    " 373 cord           -> 200 : ...\n",
    " 374 division       -> 200 : ...\n",
    " 349 formation      -> 200 : ...\n",
    " 429 phone          -> 200 : ...\n",
    "2217 product        -> 200 : 2017\n",
    " 404 text           -> 200 : ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = defaultdict(list)\n",
    "test_dict = defaultdict(list)\n",
    "\n",
    "#Your code here\n",
    "\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "assert len(test_dict.keys()) == 6\n",
    "assert len(train_dict.keys()) == 6\n",
    "assert \"product\" in test_dict.keys()\n",
    "assert \"product\" in train_dict.keys()\n",
    "assert len(test_dict[\"product\"]) ==  200\n",
    "assert len(train_dict[\"product\"]) == 2017\n",
    "assert len(train_dict[\"product\"][0]) == 49\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Creating and testing features for WSD\n",
    "\n",
    "Part 2 contains most of the work in this lab.  You will be extracting features from the semeval data you stored in Part 1.  Remember that features can be any useful information that might help in the classification.  You will be extracting several different types of features to eventually present to a classifier to do word-sense disambiguation.  You can develop them in any order, except as noted.  Be sure to coordinate with your team-mates, so you aren't all doing the same work multiple times.\n",
    "\n",
    "Note that all of the subparts here involve two steps:\n",
    "\n",
    "1. You will write a function that takes a sentence context, and returns some quantification (numbers).  i.e., you will read in the sentence context, and return the average counts of the words (this is not actually a feature here - the features you will extract are better for WSD).\n",
    "2. You will test this output of this function using your training data by averaging the numbers across all contexts for pairs of senses, seeing if an expected relationship holds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Concreteness feature\n",
    "rubric={accuracy:3,quality:1,efficiency:1}\n",
    "\n",
    "One typical distinction between senses of a word is that some senses are more concrete (involving the physical world) whereas others are more abstract.  For example, \"house\" is very concrete - it is a thing that exists in the world, while \"happiness\" is abstract - there are many different definitions, and you can't point to something and say \"that's happiness\". A list of words with human-assigned concreteness ratings can be found on the webpage [here](https://raw.githubusercontent.com/ArtsEngine/concreteness/master/Concreteness_ratings_Brysbaert_et_al_BRM.txt); the relevant column is *Conc.M*. Note that they are floating-point numbers (0 means no value was assigned). Extract this information into a Python dict (key is word, value is concreteness) and then write a function which calculates an average concreteness score for all words in a context (that is, given a list of context words, your function calculates the average concreteness of all of them). **You should lemmatize (remember the WordNet lemmatizer from COLX 521?) and lowercase the words in the context before you look them up in the dictionary.**  If a word occurs more than once, it should be counted more than once. If a word has no concreteness score (ie, Conc.M == 0) it should be left out of the calculation (both numerator and denominator).\n",
    "\n",
    "\n",
    "For example, in the sentence \"This is a test\", we get:\n",
    "\n",
    "this = 2.14 <br/>\n",
    "is = 1.59 <br/>\n",
    "a = 1.46 <br/>\n",
    "test = 3.93 <br/>\n",
    "\n",
    "So the concreteness score should be (2.14 + 1.59 + 1.46 + 3.93) / 4 = 2.28\n",
    "\n",
    "\n",
    "Then use this function to show that the \"cord\" sense of *line* appears in more concrete contexts, on average, than the \"division\" sense. You should use the function you've built, averaging the result across all the contexts for each of those two senses (using the training data from part 1). \n",
    "\n",
    "Remember that you've already collected all the contexts for each sense in part 1.  In this part, you'll be calculating the average concreteness score for each sense - calculate the concreteness score for each context of the sense, and then calculate the average of those scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brysbaert, M., Warriner, A.B. & Kuperman, V.  (2014).\n",
    "Concreteness ratings for 40 thousand generally known English word lemmas. *Behavior Research Methods*. 46:904â€“911. https://doi.org/10.3758/s13428-013-0403-5 \n",
    "\n",
    "**Conc.M**:\n",
    "```\n",
    "Word\t\tBigram\tConc.M\tConc.SD\tUnknown\tTotal\tPercent_known\tSUBTLEX\tDom_Pos\n",
    "roadsweeper\t0\t4.85\t0.37\t1\t27\t0.96\t0\t0\n",
    "traindriver\t0\t4.54\t0.71\t3\t29\t0.90\t0\t0\n",
    "tush\t\t0\t4.45\t1.01\t3\t25\t0.88\t66\t0\n",
    "...\n",
    "```\n",
    "\n",
    "`# of features = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ArtsEngine/concreteness/master/Concreteness_ratings_Brysbaert_et_al_BRM.txt\", delimiter=\"\\t\", index_col=0)\n",
    "conc_dict = df[\"Conc.M\"].to_dict()\n",
    "\n",
    "def get_conc_score(context):\n",
    "    '''calculate the average concreteness score for all words in a given context'''\n",
    "    valid_context = []\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    \n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "test_context = pos_tag(word_tokenize(\"I have a cat\"))\n",
    "assert get_conc_score(test_context) == ((2.18 + 1.46 + 4.86) / 3)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concreteness score for 'cord' is: 2.722\n",
      "The concreteness score for 'division' is: 2.451\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Calculate the average concreteness of all contexts of the sense \"cord\" and \"division\".  Show that\n",
    "\"cord\" is higher.\n",
    "'''\n",
    "\n",
    "cord_context_conc = 0\n",
    "#Your code here\n",
    "\n",
    "#Your code here\n",
    "print(\"The concreteness score for 'cord' is: \" + str(round(cord_context_conc, 3)))\n",
    "\n",
    "div_context_conc = 0\n",
    "#Your code here\n",
    "\n",
    "#Your code here\n",
    "print(\"The concreteness score for 'division' is: \" + str(round(div_context_conc, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Gloss overlap features (Lesk)\n",
    "rubric={accuracy:4,efficiency:1,quality:1}\n",
    "\n",
    "In this part you're going to apply the Lesk approach to WSD, looking for word overlap between the gloss of the sense and the context. However, you're not going to be able to use the version included in WordNet, for two reasons:\n",
    "\n",
    "1. We will be using a restricted set of senses, not all possible senses for *line* included in WordNet\n",
    "2. Rather than a single feature indicating which sense was chosen, we are going to calculate an overlap score for each possible sense\n",
    "\n",
    "To apply Lesk, you will first need to associate each sense in the Senseval dataset with a synset in WordNet. I've attached the most-likely synset to each sense in the Senseval dataset.  You are free to look at the definitions of the senses, and see how I arrived at those definitions.\n",
    "\n",
    "\n",
    "Write a function which takes a sentence context, and calculates the number of tokens that overlap between the context and the gloss of each sense in WordNet (HINT: use set intersection - we are only interested in *type* overlap). Your overlap calculation should exclude English stopwords (see COLX 521 Lecture 2). Your function should return a dictionary where the keys are senses and the values are overlap counts.\n",
    "\n",
    "Then, show that the average overlap of the \"product\" gloss from WordNet is higher with \"product\" contexts than \"division\" contexts.  Again, use the training data from Part 1.  At this point, you'll have a synset_dictionary (with the glosses from WordNet), and a list of contexts for each sense.  You can use these to calculate the average overlap of each sense in your context dictionary.\n",
    "\n",
    "For example, if your context dictionary has \"This line is busy\" and \"Hold the line\" for the sense 'phone', then you can calculate the overlap of \"This line is busy\" with the synset gloss of \"phone\", the same thing for the line \"Hold the line\", and then average them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lesk, Michael. (1986).\n",
    "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. \n",
    "*Proceedings of SIGDOC '86*, 24â€“26. https://doi.org/10.1145/318723.318728 \n",
    "\n",
    "\n",
    "|`wn.synset('line.n.18').definition()`    |  |  `SensevalInstance(word='line-n', position=16, context=[('perhaps', 'RB'), ('not', 'RB'), ('surprisingly', 'RB'), (',', ','), ('the', 'DT'), ('locals', 'NNS'), ... ], senses=('cord',))` |\n",
    "|---|---|---|\n",
    "| *something (as a cord or rope) that is long and thin and flexible*   |  $\\bigcap$  | *perhaps not surprisingly , the locals often call the warden . while noodlers generally drag a line with ...*  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cord', 'division', 'formation', 'phone', 'product', 'text'])\n"
     ]
    }
   ],
   "source": [
    "print(train_dict.keys())\n",
    "line_synsets = wn.synsets(\"line\")\n",
    "synset_lookup = {}\n",
    "synset_lookup['cord'] = wn.synset('line.n.18')\n",
    "synset_lookup['division'] = wn.synset('line.n.29')\n",
    "synset_lookup['formation'] = wn.synset('line.n.01')\n",
    "synset_lookup['phone'] = wn.synset('telephone_line.n.02')\n",
    "synset_lookup['product'] = wn.synset('line.n.22')\n",
    "synset_lookup['text'] = wn.synset('line.n.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line.n.01\n",
      "a formation of people or things one beside another\n",
      "line.n.02\n",
      "a mark that is long relative to its width\n",
      "line.n.03\n",
      "a formation of people or things one behind another\n",
      "line.n.04\n",
      "a length (straight or curved) without breadth or thickness; the trace of a moving point\n",
      "line.n.05\n",
      "text consisting of a row of words written across a page or computer screen\n",
      "line.n.06\n",
      "a single frequency (or very narrow band) of radiation in a spectrum\n",
      "line.n.07\n",
      "a fortified position (especially one marking the most forward position of troops)\n",
      "argumentation.n.02\n",
      "a course of reasoning aimed at demonstrating a truth or falsehood; the methodical process of logical reasoning\n",
      "cable.n.02\n",
      "a conductor for transmitting electrical or optical signals or electric power\n",
      "course.n.02\n",
      "a connected series of events or actions or developments\n",
      "line.n.11\n",
      "a spatial location defined by a real or imaginary unidimensional extent\n",
      "wrinkle.n.01\n",
      "a slight depression in the smoothness of a surface\n",
      "pipeline.n.02\n",
      "a pipe used to transport liquids or gases\n",
      "line.n.14\n",
      "the road consisting of railroad track and roadbed\n",
      "telephone_line.n.02\n",
      "a telephone connection\n",
      "line.n.16\n",
      "acting in conformity\n",
      "lineage.n.01\n",
      "the descendants of one individual\n",
      "line.n.18\n",
      "something (as a cord or rope) that is long and thin and flexible\n",
      "occupation.n.01\n",
      "the principal activity in your life that you do to earn money\n",
      "line.n.20\n",
      "in games or sports; a mark indicating positions or bounds of the playing area\n",
      "channel.n.05\n",
      "(often plural) a means of communication or access\n",
      "line.n.22\n",
      "a particular kind of product or merchandise\n",
      "line.n.23\n",
      "a commercial organization serving as a common carrier\n",
      "agate_line.n.01\n",
      "space for one line of print (one column wide and 1/14 inch deep) used to measure advertising\n",
      "credit_line.n.01\n",
      "the maximum credit that a customer is allowed\n",
      "tune.n.01\n",
      "a succession of notes forming a distinctive sequence\n",
      "line.n.27\n",
      "persuasive but insincere talk that is usually intended to deceive or impress\n",
      "note.n.02\n",
      "a short personal letter\n",
      "line.n.29\n",
      "a conceptual separation or distinction\n",
      "production_line.n.01\n",
      "mechanical system in a factory whereby an article is conveyed through sites at which successive operations are performed on it\n",
      "line.v.01\n",
      "be in line with; form a line along\n",
      "line.v.02\n",
      "cover the interior of\n",
      "trace.v.02\n",
      "make a mark or lines on a surface\n",
      "line.v.04\n",
      "mark with lines\n",
      "line.v.05\n",
      "fill plentifully\n",
      "line.v.06\n",
      "reinforce with fabric\n",
      "SIZE:  36\n"
     ]
    }
   ],
   "source": [
    "for synset in line_synsets:\n",
    "    print(synset.name())\n",
    "    print(synset.definition())\n",
    "print(\"SIZE: \", len(line_synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_overlap(context):\n",
    "    '''Calculate the number of tokens that overlap between the context and the gloss of each sense in WordNet'''\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = \"I was holding a flexible line\"\n",
    "test_context = pos_tag(word_tokenize(test_sent))\n",
    "assert count_overlap(test_context)['cord'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average overlap of the 'product' gloss with the product contexts is: 0.059\n",
      "The average overlap of the 'product' gloss with the division contexts is: 0.029\n",
      "{'product': 0.059494298463063956, 'division': 0.028735632183908046}\n"
     ]
    }
   ],
   "source": [
    "# 'Product' gloss overlap with product contexts\n",
    "avg_overlap_dict = {}\n",
    "avg_overlap_dict['product'] = 0\n",
    "#Your code here\n",
    "    \n",
    "#Your code here\n",
    "\n",
    "avg_overlap_dict['division'] = 0\n",
    "#Your code here\n",
    "    \n",
    "#Your code here\n",
    "    \n",
    "print(f\"The average overlap of the 'product' gloss with the product contexts is: {avg_overlap_dict['product']:0.3f}\")\n",
    "print(f\"The average overlap of the 'product' gloss with the division contexts is: {avg_overlap_dict['division']:0.3f}\")\n",
    "print(avg_overlap_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 : WordNet distance features\n",
    "rubric={accuracy:5,efficiency:1,quality:1}\n",
    "\n",
    "This feature involves calculating the WordNet (Wu-Palmer) distances from the synsets of relevant senses of *line* to the synsets of mostly non-ambiguous context words. For this, you will need the Senseval -> WordNet sense mapping from 2.2.  If you can't remember how to get the Wu-Palmer (ie, wup) value, check the lecture slides.\n",
    "\n",
    "The biggest challenge in this problem is identifying \"mostly\" non-ambiguous words. We could exclude any word type that has any polysemy (i.e. associated with more than one synset), but that seems too extreme (almost all words have some rare instances of strange sense uses). Instead, we are going to consider a word mostly non-ambiguous if it appears as one particular sense 75% of the time, based on the corpus counts provided in WordNet. You should write a general function, `get_dominant_sense`, which takes a word and a POS (a single letter, same as the input to the WordNet lemmatizer), and returns the dominant (75% of instances) synset if it exists, or `None` if it doesn't. The POS will be useful because, in order to do this properly, you will have to correctly lemmatize the word, so as to match it with the lemmas of each of its synset, so you can get the right count.\n",
    "\n",
    "So this function should take a word and pos as input, and then: <br/>\n",
    "1. Lemmatize the word <br/>\n",
    "2. Get all the senses from the WordNet synsets for the word <br/>\n",
    "3. Keep track of the counts for each sense that match the lemma <br/>\n",
    "4. If the highest count is greater than 0.75 * total count, then return that synset.  Otherwise, return None. <br/>\n",
    "\n",
    "Once you have this function, you should create another function which will, for a particular instance,\n",
    "\n",
    "1. Use `get_dominant_sense` to get a list of synsets appearing in the context (one for each mostly non-ambiguous word). You will need to do this again in 2.4, so a separate function might be a good idea!  The function will take a context as input, and return a list of synsets.\n",
    "2. For each sense of *line* in Senseval (ie, the senses in synset_lookup), calculate the average distance between that sense and all the synsets in the context. You should use the built-in function for calculating Wu-Palmer distance between a synset pair, don't implement your own.  \n",
    "3. Return a dictionary mapping the (Senseval) sense to the average distance to the context synset.  That is, return a dictionary where the keys are the six senses in synset_lookup, and the values are the average distance from the context to that sense.\n",
    "\n",
    "Then use the output of this function to show that the synsets associated with contexts around \"phone\" sense of line are on average closer to the \"phone\" synset than synsets from \"division\" contexts are.\n",
    "\n",
    "That is, for each context in your training dictionary with the sense \"phone\", calculate the average distance between the context and the \"phone\" sense in synset_lookup.  Then, do the same for each context in your training dictionary with the sense \"division\".  Show that the \"phone\" sense is closer for \"phone\" contexts than \"division\" contexts (\"closer\" means the number will be smaller - this is a distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\"> \n",
    "Wu, Z., & Palmer, M. (1994). \n",
    "Verb Semantics and Lexical Selection. <i>Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics</i>, 133â€“138. https://doi.org/10.3115/981732.981751\n",
    "</span>\n",
    "\n",
    "$\\text{Wu-Palmer} = \\displaystyle\\frac{2 \\cdot \\text{depth}(LCS)}{\\text{depth}(sn_1) + \\text{depth}(sn_2)}$\n",
    "where $LCS$ = least common subsumer and $sn$ = synset. \n",
    "\n",
    "```\n",
    ">>> car = wordnet.synset('car.n.01')\n",
    ">>> boat = wordnet.synset('boat.n.01')\n",
    ">>> car.wup_similarity(boat)\n",
    "0.6956521739130435\n",
    "\n",
    "    Object\n",
    "      |\n",
    "    Vehicle ***\n",
    "      |\n",
    "  ---------\n",
    "  |       |\n",
    " Boat  Automobile\n",
    "          |\n",
    "         Car\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> hyp = lambda s:s.hypernyms()\n",
    ">>> from pprint import pprint\n",
    ">>> pprint(car.tree(hyp))\n",
    "[Synset('car.n.01'),\n",
    " [Synset('motor_vehicle.n.01'),\n",
    "  [Synset('self-propelled_vehicle.n.01'),\n",
    "   [Synset('wheeled_vehicle.n.01'),\n",
    "    [Synset('container.n.01'),\n",
    "        ...\n",
    "    [Synset('vehicle.n.01'), ***\n",
    "     [Synset('conveyance.n.03'),\n",
    "      [Synset('instrumentality.n.03'),\n",
    "       [Synset('artifact.n.01'),\n",
    "        [Synset('whole.n.02'),\n",
    "         [Synset('object.n.01'),\n",
    "          [Synset('physical_entity.n.01'),\n",
    "           [Synset('entity.n.01')]]]]]]]]]]]]\n",
    ">>> pprint(boat.tree(hyp))\n",
    "[Synset('boat.n.01'),\n",
    " [Synset('vessel.n.02'),\n",
    "  [Synset('craft.n.02'),\n",
    "   [Synset('vehicle.n.01'), ***\n",
    "    [Synset('conveyance.n.03'),\n",
    "     [Synset('instrumentality.n.03'),\n",
    "      [Synset('artifact.n.01'),\n",
    "       [Synset('whole.n.02'),\n",
    "        [Synset('object.n.01'),\n",
    "         [Synset('physical_entity.n.01'), \n",
    "          [Synset('entity.n.01')]]]]]]]]]]]\n",
    ">>> 2*8 / (12+11)\n",
    "0.6956521739130435\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate Wu-Palmer, you need to get the dominant sense of each word by using `get_dominant_sense(\"word\",\"n\")` which can generate as follows:\n",
    "\n",
    "\n",
    "```\n",
    "[\n",
    " [0, Synset('password.n.01')],   <-- Lemma('password.n.01.word').count() = 0\n",
    " [0, Synset('word.n.07')], \n",
    " [1, Synset('parole.n.01')], \n",
    " [2, Synset('give_voice.v.01')], \n",
    " [3, Synset('discussion.n.02')], \n",
    " [3, Synset('word.n.04')], \n",
    " [5, Synset('news.n.01')], \n",
    " [18, Synset('word.n.02')], \n",
    " [117, Synset('word.n.01')]      <-- dominant sense\n",
    "]\n",
    "117/149 = 0.785234899328859 => 0.75\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_sense_ratio = 0.75\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_dominant_sense(word,pos=\"n\"):\n",
    "    '''return the dominant (75% of instances) synset of the word if it exists, or None if it doesn't\n",
    "    word -- an English word\n",
    "    pos -- a single letter that represents part of speech of the input word, noun by default'''\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_dominant_sense('word', 'n').name() == 'word.n.01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_sense_context(context):\n",
    "    '''return a list of dominant synsets appearing in the context,\n",
    "    one for each mostly non-ambiguous word'''\n",
    "    valid_context = []\n",
    "    dominant_synsets = []\n",
    "    \n",
    "    #Your code here\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_distance(context):\n",
    "    '''calculate average distance between senses of the word \"line\" and a context'''\n",
    " \n",
    "    #Your code here\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the synsets associated with contexts around 'phone' sense of line and 'phone' synset:  0.758\n"
     ]
    }
   ],
   "source": [
    "avg_phone_distance = 0\n",
    "count = 0\n",
    "\n",
    "#Your code here\n",
    "\n",
    "#Your code here\n",
    "print(\"The distance between the synsets associated with contexts around 'phone' sense of line and 'phone' synset: \", round(avg_phone_distance / count, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the synsets associated with contexts around 'division' sense of line and 'phone' synset:  0.798\n"
     ]
    }
   ],
   "source": [
    "avg_division_distance = 0\n",
    "count = 0\n",
    "\n",
    "#Your code here\n",
    "\n",
    "#Your code here\n",
    "print(\"The distance between the synsets associated with contexts around 'division' sense of line and 'phone' synset: \", round(avg_division_distance / count, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 WordNet Hypernyms\n",
    "rubric={accuracy:5, efficiency:1, quality:1}\n",
    "\n",
    "Now, we will consider the count of WordNet synsets in the context directly as features. However, limiting ourselves to the synsets corresponding directly to words might result in sparsity, and provide little more information than raw words would. Instead, we are going to also include all the hypernyms of words appearing in the context as potential features for doing WSD.\n",
    "\n",
    "First, write a recursive function `get_all_hypernyms` which collects the names (e.g. `synset.name()`) of a provided WordNet synset and all of its hypernyms.  The base case can just be when an item no longer has any hypernyms.\n",
    "\n",
    "Then, applying this function to the synsets found in the context (step 1 of the distance function in 2.3), write a function that counts all the hypernyms of all the (again mostly non-ambiguous) synsets in the context, normalizing by the total count to get a proportion for each synset.\n",
    "\n",
    "The function will take a context as input.  It will calculate the dominant synsets from this context.  Then, for each of these synsets, it will get all of their hypernyms, and keep track of their counts.\n",
    "The returned dictionary will have the hypernym names as keys, and the percentage of all the hypernyms found using this method.  For example, if you count all the hypernyms, and have 20, and 5 of them are \"animal\", then \"animal\" will have a value of \"0.25\"\n",
    "\n",
    "Then show that the average proportion of the 'object.n.01' synset is higher in contexts involving the \"cord\" sense of *line* than the \"division\" sense. (This should be true for the same reason as in 2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> from nltk.corpus import wordnet as wn\n",
    ">>> wn.synsets('cat')\n",
    "[Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), \n",
    "Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), \n",
    "Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), \n",
    "Synset('vomit.v.01')]\n",
    ">>> wn.synset('cat')    <-- not OK\n",
    "...\n",
    "ValueError: not enough values to unpack (expected 3, got 1)\n",
    ">>> wn.synset('cat.n.01')\n",
    "Synset('cat.n.01')\n",
    ">>> wn.synsets('cat')[0]\n",
    "Synset('cat.n.01')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hypernyms(synset, names=[]):\n",
    "    '''return a list of the names of a synset and all its hypernyms'''\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_all_hypernyms(wn.synsets(\"cat\")[0])`\n",
    "where `wn.synsets(\"cat\")[0]` (or `wn.synset(\"cat.n.01\")`) is `get_dominant_sense(\"cat\",\"n\")`:\n",
    "```\n",
    "['cat.n.01', 'feline.n.01', 'carnivore.n.01', 'placental.n.01', 'mammal.n.01', 'vertebrate.n.01', 'chordate.n.01', 'animal.n.01', 'organism.n.01', 'living_thing.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_synset = wn.synset(\"cat.n.01\")\n",
    "assert get_all_hypernyms(cat_synset, []) == [\n",
    "    \"cat.n.01\",\n",
    "    \"feline.n.01\",\n",
    "    \"carnivore.n.01\",\n",
    "    \"placental.n.01\",\n",
    "    \"mammal.n.01\",\n",
    "    \"vertebrate.n.01\",\n",
    "    \"chordate.n.01\",\n",
    "    \"animal.n.01\",\n",
    "    \"organism.n.01\",\n",
    "    \"living_thing.n.01\",\n",
    "    \"whole.n.02\",\n",
    "    \"object.n.01\",\n",
    "    \"physical_entity.n.01\",\n",
    "    \"entity.n.01\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_of_all_hypernyms(context):\n",
    "    '''get all the hypernyms of all the dominent synsets in a given context,\n",
    "       return normalized counts'''\n",
    "    \n",
    "    #Your code here\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heard.s.01': 0.04,\n",
       " 'struggling.s.01': 0.04,\n",
       " 'rebuff.n.01': 0.04,\n",
       " 'discourtesy.n.03': 0.04,\n",
       " 'behavior.n.01': 0.04,\n",
       " 'activity.n.01': 0.04,\n",
       " 'act.n.02': 0.04,\n",
       " 'event.n.01': 0.04,\n",
       " 'psychological_feature.n.01': 0.04,\n",
       " 'abstraction.n.06': 0.04,\n",
       " 'entity.n.01': 0.12,\n",
       " 'beach.n.01': 0.04,\n",
       " 'geological_formation.n.01': 0.04,\n",
       " 'object.n.01': 0.08,\n",
       " 'physical_entity.n.01': 0.08,\n",
       " 'bucket.n.01': 0.04,\n",
       " 'vessel.n.03': 0.04,\n",
       " 'container.n.01': 0.04,\n",
       " 'instrumentality.n.03': 0.04,\n",
       " 'artifact.n.01': 0.04,\n",
       " 'whole.n.02': 0.04}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_of_all_hypernyms(train_dict['cord'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(get_all_of_all_hypernyms(train_dict['cord'][16]).values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On average, are the synsets associated with contexts around \"phone\" sense of `line` closer to the \"phone\" synset than synsets from \"division\" contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg proportion of the 'object.n.01' synset in 'cord' contexts:  0.062\n"
     ]
    }
   ],
   "source": [
    "# average proportion of the \"object.n.01\" synset in \"cord\" contexts\n",
    "\n",
    "total_prop = 0\n",
    "count = 0\n",
    "for context in train_dict['cord']:\n",
    "    hypernyms = get_all_of_all_hypernyms(context)\n",
    "    if \"object.n.01\" in get_all_of_all_hypernyms(context).keys():\n",
    "        total_prop += hypernyms[\"object.n.01\"]\n",
    "        count += 1\n",
    "\n",
    "print(\"avg proportion of the 'object.n.01' synset in 'cord' contexts: \", round(total_prop / count, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg proportion of the 'object.n.01' synset in 'division' contexts:  0.042\n"
     ]
    }
   ],
   "source": [
    "# average proportion of the \"object.n.01\" synset in \"division\" contexts\n",
    "\n",
    "total_prop = 0\n",
    "count = 0\n",
    "for context in train_dict['division']:\n",
    "    hypernyms = get_all_of_all_hypernyms(context)\n",
    "    if \"object.n.01\" in hypernyms.keys():\n",
    "        total_prop += hypernyms[\"object.n.01\"]\n",
    "        count += 1\n",
    "\n",
    "print(\"avg proportion of the 'object.n.01' synset in 'division' contexts: \", round(total_prop / count, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Building a classifier\n",
    "rubric={accuracy:2,quality:1,reasoning:1}\n",
    "\n",
    "Now that we have a collection of features which show some promise for the task, you should build a classifier for WSD of *line* which uses all the features above. As before, you should use DictVectorizers with descriptive feature names, combining all the individual outputs of the functions into a single feature dictionary for each context sentence. Note that there is 1 feature from 2.1, 6 features each from 2.2 and 2.3 (one for each sense of \"line\") and 2.4 involves thousands of features. The classifications here are the Senseval senses. You can again just use a Decision Tree (with more features than in Lab 4 of COLX 521, you should probably try a higher max_depth, e.g. 5), though you're also welcome to use something else as well. In addition to outputting the performance on the classifier on the test set using all features, please do one interesting experiment that checks the test performance using a feature set other than all the features, and briefly discuss the results. (The full ablation/feature selection is NOT necessary here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(context):\n",
    "    '''Extract a feature dictionary for an input text'''\n",
    "    feature_dict = {}\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_dicts = []\n",
    "train_classification = []\n",
    "test_feat_dicts = []\n",
    "test_classification = []\n",
    "\n",
    "for key in test_dict.keys():\n",
    "    for context in range(len(test_dict[key])):\n",
    "        test_feat_dicts.append(get_feature_dict(test_dict[key][context]))\n",
    "        test_classification.append(key)\n",
    "\n",
    "try:\n",
    "    for key in train_dict.keys():\n",
    "        for context in range(len(train_dict[key])):\n",
    "            train_feat_dicts.append(get_feature_dict(train_dict[key][context]))\n",
    "            train_classification.append(key)\n",
    "except IndexError:\n",
    "    print(IndexError)\n",
    "    print(train_dict[key][context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train_dict, test_dict):\n",
    "    '''vectorize given lists of feature dictionaries, return X_train and X_test'''\n",
    "    vectorizer = DictVectorizer(sparse=False, dtype=float)\n",
    "    X_train = vectorizer.fit_transform(train_dict)\n",
    "    X_test = vectorizer.transform(test_dict)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = vectorize(train_feat_dicts, test_feat_dicts)\n",
    "y_train, y_test = train_classification, test_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score:  0.2225\n"
     ]
    }
   ],
   "source": [
    "print(\"Final score: \", tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment: Taking out `average_distance` features (WordNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_average_distance_feature(feat_dicts):\n",
    "    '''remove average distance feature from data set'''\n",
    "    new_feat_dicts = []\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "    return new_feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_feat_dicts, new_test_feat_dicts = remove_average_distance_feature(train_feat_dicts), remove_average_distance_feature(test_feat_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_test_feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_train, X_new_test = vectorize(new_train_feat_dicts, new_test_feat_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "tree.fit(X_new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with new training set:  0.2275\n"
     ]
    }
   ],
   "source": [
    "print(\"Score with new training set: \", tree.score(X_new_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to take out the average distance feature from the data sets because it was the feature that showed the least significant distinction between different senses of contexts. As a result, we got a slightly higher score compared to the final score with all features. Given that feature extraction takes a significant amount of time, it would be better to take out the feature from the data sets and try to implement other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Teamwork report\n",
    "rubric={raw:1, reasoning:1}\n",
    "\n",
    "Briefly discuss how each person contributed to the project. Though it is not necessary that every group member has a equal contribution in terms of code, every group member should have a significant contribution, a major part of the lab for which they were a primary contributor. If any team member fails to contribute significantly, the rest of the group, assuming they have made efforts to encourage the team member to contribute, should discuss what happened in this report, otherwise all team members might lose raw points for this rubric. Team members who failed to contribute may lose some or all of their grade for the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Extra Feature(s) (Optional)\n",
    "rubric={raw:2}\n",
    "\n",
    "Develop another feature or feature set to try to improve your WSD performance. This could be an variation on or combination of the features from above, or perhaps something totally new. However, in order to get any points, **it must rely in some key way on information from WordNet**. You will be graded based on novelty, programming difficulty, and how much improvement you see on on the test set (you can still get some points for an interesting idea that fails). You can try multiple distinct features or feature types, but you'll only get points for the best one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
