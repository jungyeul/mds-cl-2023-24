{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 521 Lab Assignment 4: L2 Learner Classifier Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will \n",
    "- preprocess text derived from HTML documents\n",
    "- extract corpus linguistic features\n",
    "- Built a supervised classification system to identify language background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be creating an end-to-end classification system using a corpus of HTML documents scraped from the [Lang-8 language learning website](https://lang-8.com/). In particular, you will be building a classifier to distinguish English text written by Lang-8 users whose native language (L1) is another European language (French and Spanish) from those written by L1 speakers of East Asian languages (Japanese, Korean, and Mandarin Chinese). This is a group project where your team will submit a single solution, see details on submissions below.\n",
    "\n",
    "A bit of historical linguistics: Although French and Spanish are closely related languages, being both part of the Romance branch of the Indo-European languages, Japanese, Korean and Mandarin are not considered related by linguists. Therefore, from a purely linguistics perspective our task is not particularly well formed. However, there is a key distinction between European and East Asian languages in terms of their relationship to the \"L2\" (second language) in this case, English. Although English is in a different branch of the Indo-European language family (Germanic), Spanish and French nonetheless share numerous linguistic similarities with English (particularly with regards to having a shared educated vocabulary), whereas the East Asian Languages, having developed quite separately, have only coincidental similarities with English. One might presume this linguistic distance generally makes English more challenging for East Asian learners, who are in essence \"starting from scratch\" in a way that Spanish and French speakers are not. You may want to consider this as you brainstorm possible features! You might also reflect on general cultural differences between East Asia and Europe, and how they might be reflected in linguistic features you can easily extract. Or you can put aside these sorts of assumptions, and just let the data lead you! It is up to you.\n",
    "\n",
    "You will probably want to divide this project among members of your team according to their skills, schedule, and interest. Everyone must make a significant contribution. Although everything in the project is to some degree dependent on the rest, it is quite feasible to write most of the code for each part without having any of the other parts yet complete/working, provided you agree on the interface between the parts. For example, once you agree on the exact representation for each text, different team members can write functions which extract features for Part 2 even if the preprocessing code in Part 1 which creates that representation isn't yet ready. Talk to your group early, and figure out how you are going to tackle it. **Any student which has failed to get in touch with their group before Thursday will lose 25% percent of the grade per day, starting on Thursday.**\n",
    "\n",
    "Before you start serious work on this project, be sure to read **all** the instructions below, so you have a clear idea of the big picture in terms of what you are trying to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the marks for tidy submission:\n",
    "\n",
    "- One person in each group must create a private UBC github repo, and give access to all group members as well as the instructor (@gnicolai), lab instructor (@jungyeul) and TA (@siliang6)\n",
    "- In the readme in the individual lab repo (the one created when the lab is opened) for all members of the group, you should have a link to this private, shared repo. Pushing that link is your only \"submission\". Don't put anything else in your repo for this lab.\n",
    "- In the readme of the private shared repo, include instructions that will allow someone to reproduce your results and identify the parts of the code relevant to the major sections discussed below. \n",
    "- You do NOT have to use the ipynb you are reading; You may find it useful to have multiple ipynbs (or even standalone .py files which your .ipynb(s) import) for different parts of the assignment, just make sure you indicate how your code should be run (You should assume it will be run). You can also just add cells to this ipynb, whatever works for you.\n",
    "- Note that any commits on the private shared repo after the deadline will result in a late penalty being applied to the project, so be careful about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text extraction from HTML\n",
    "rubric = {accuracy:4,efficiency:1,quality:2}\n",
    "\n",
    "You first need to write code to extract relevant information from a large collection of HTML documents from the Lang-8 language learning website (lang-8.zip), available in the students repo [here](https://github.ubc.ca/MDS-CL-2023-24/COLX_521_corp-ling_students/tree/master/data/lang-8.zip). You should start by downloading this corpus and putting it in your shared repo. You may choose to extract the files from the zip manually, or you can pull the documents from the zip file within your code using the `zipfile` package (see discussion in Lecture 8). Both are okay, though the zipfile version is slightly preferred since having thousands of files on github is a bit messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each html file consists of an online diary entry written by a second language learner of English. There are three things that your code will need to pass on to next part of the process, for each file:\n",
    "\n",
    "- the native language of the writer\n",
    "- the raw text (string) of the entry, with HTML removed\n",
    "- the original filename (so that you can use our provided train/dev/test split) \n",
    "\n",
    "You'll need to inspect a sample file to identify where this information is contained in the HTML, and then write code to pull it out.\n",
    "\n",
    "Quality and efficiency tips:\n",
    "- Don't forget to document!\n",
    "- There are a couple of fairly independent tasks here, so you'll probably want multiple functions\n",
    "- However, what can be shared among functions, should be shared (i.e. don't have two functions load the same file twice!).\n",
    "- Ideally, you main function which iterates over all the documents should be a generator (with a `yield` statement) that allows Part 2 code to process the documents one by one, rather than having the entire corpus in memory at once. \n",
    "- If you choose to have multiple ipynbs, however, you can also choose instead to save the info you need from each file (not necessarily in separate files, you might keep it all in one file) and load it separately in the Part 2 ipynb. As long as you do this in a way that doesn't involving keep the whole corpus in memory, you can still get full efficiency points.\n",
    "- You will want to make appropriate use of packages we have talked about in this course and their methods, which can save you a lot of work (e.g. BeautifulSoup's `get_text`)\n",
    "- Create a few tests for each function you write.\n",
    "- You can build this all into a class if you'd like, but that isn't necessary for full quality points, functions are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L1(html):\n",
    "    '''Extract the native language from the html string'''\n",
    "\n",
    "def get_text(html):\n",
    "    '''Extract the entry text from the html string'''\n",
    "    # \"div\" shoud have \"id\" with \"body_show_ori\"\n",
    "\n",
    "def get_filename_in_txt(txt_file):\n",
    "    '''Return a set of strings as filenames in txt file'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Extraction\n",
    "rubric = {accuracy:6,efficiency:1,quality:2}\n",
    "\n",
    "The next step is to go from a general text representation to a set of features for each text, in the right format to be fed into Scikit-Learn. \n",
    "\n",
    "- You will first want to apply general text preprocessing techniques (Lecture 7) to get a useful Python data structure that isn't just raw text (it's also okay if you did this in the code included in Part 1!).\n",
    "- Then you should extract some features from the texts. You have significant leeway which features you extract, but for full points you need to follow the rules below:\n",
    "    - Your final system **can only have 10 features**. You may choose to test more than that using the dev set, but you will have to drop some before you do your final test. Do NOT use bag-of-word features here!\n",
    "        - Any feature which has a categorical (string) value is actually a different feature for each categorical value (it becomes \"one-hot\", if you've heard that term before). All your features for this project should probably have values that are either numerical or boolean, which result in a single feature.\n",
    "    - You must have at least one boolean feature based on a lexicon, meaning a feature which is True (or 1) if a word from a set of words you decide on is somewhere in the text, or False (0) otherwise. Some more rules for your lexicon(s):\n",
    "         - A lexicon must contain at least three words\n",
    "         - If it contains more than 10 words, you must store it in a separate text file and load it \n",
    "         - Although you may use purely statistical means (i.e. see \"comparing corpora\" in Lecture 4) to identify promising words, the lexicon must have an interpretable meaning other than a set of words that make the classifier do better. There needs to be something that ties all these lexical items together in a human interpretable way. E.g. {\"car\", \"truck\", \"train\", \"boat\"} are all modes of transport.\n",
    "    - You must have at least one \"statistic\", of the sort we saw in Lecture 4, one that is **not** just the count of a specific lexical item (or a lexicon of items) in a text.\n",
    "    - You must have one feature that involves doing sentence segmentation\n",
    "    - You must have one feature that involves doing part-of-speech tagging\n",
    "    - You must have one feature that involves doing lemmatization or stemming \n",
    "    - A single feature can satisfy more than one of the requirements above. For example, you might have a statistic that relies on sentence segmentation, satisfying both the statistic and sentence segmentation requirement.\n",
    "    - You can do preprocessing as part of your feature extraction. For example, POS tagging and lemmatization are actually somewhat incompatible, and so you may want to do those independently\n",
    "    - If you look at the corpus to develop your features (a good idea) you should make sure that you look in the training set, not the dev or test sets.\n",
    "\n",
    "- As you are building these feature dictionaries (your \"X\"), you will be also be creating the list of classifications (your \"y\"). You should map those texts with L1 either Spanish or French to the \"European\" classification, and Japanese, Korean, and Mandarin to the \"Asian\" classification. There are also Russian texts in the corpus, but we are not going to use them.\n",
    "- Before you create the feature dictionaries, you will want to load the list of filenames found in `train.txt`, `dev.txt`, and `test.txt` files (in the same directory as `lang8.zip`, [here](https://github.ubc.ca/MDS-CL-2022-23/COLX_521_corp-ling_students/tree/master/data)) and use them to create 3 separate lists of feature dicts (and corresponding lists of classifications, i.e. \"European\" and \"Asian\") for each of the train/dev/test splits, using the filename returned from Part 1. Note that the Russian texts we are not using are included in the splits, so you should not create the splits by iterating over train/dev/text.txt, but rather iterate over all the documents in the corpus and put the extracted features for relevant texts in the appropriate list, based on whether they are in the lists of train, dev, or test sets you have already loaded beforehand.\n",
    "  \n",
    " Quality and efficiency tips:\n",
    " \n",
    " - Again, don't forget to document!\n",
    " - You will probably want to have an independent function for extracting each particular feature from one text, unless...\n",
    " - If you have multiple features that can be generalized into a single function, that can be good too!\n",
    " - Again, make sure things that can be shared are shared (e.g. POS tagging), don't redo the same processing for multiple features\n",
    " - Make your feature names easily interpretable, even if they're a bit long\n",
    " - In addition to the individual functions for each feature, you'll probably want to have a single function that builds a feature dictionary for one text by calling those functions.\n",
    " - You should have individual tests that show each feature extraction function is working correctly!\n",
    " - Make sure you convert the filenames listed in `train.txt`, `dev.txt`, and `test.txt` into sets, so that looking up which the correct \"split\" for each lang-8 entry is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what features? \n",
    "\n",
    "# Get the average sentence length\n",
    "# Get the total text length\n",
    "# Get the number of sentences in text\n",
    "# Get relative frequency of prepositions for each document\n",
    "# Get the type token ratio for a text\n",
    "\n",
    "# Get reading ease (see Lab2)\n",
    "# Get percentage of any POS in document \n",
    "# Get the relative frequency of modal verbs\n",
    "# Get relative frequency of articles for each document\n",
    "# Get relative frequency of adjectives and adverbs for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "rubric = {accuracy:4,efficiency:1,quality:1}\n",
    "\n",
    "Now you're going to build and test a machine learning model based on your features\n",
    "\n",
    "- First, vectorize your lists of feature dictionaries appropriately, turning them into first into sparse matrices, and then numpy arrays (using `toarray`). With so few features, you don't have to worry about keeping things sparse, and having dense arrays will be useful later.\n",
    "- Build a sklearn DecisionTreeClassifier with max_depth=3 (for regularization), and get a preliminary [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) on the _dev_ set using all features. You may choose to modify the classifier and hyperparameters settings to get better results, see discussion in \"Part 5\" below.\n",
    "- Next, you're going to do feature [ablation](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence%29), which involves removing features one at a time from your model, and seeing what the effect is. Report results with each feature missing (make it clear which feature is missing for each result you present). If the results go down when you remove the feature, it means the feature is probably useful, and if the results go up or stay the same, then it is probably not.\n",
    "- The last step of the coding part of the lab is checking accuracy of your model on the _test_ set, after removing any features that are not useful based on _dev_ set results. However, only run this step when you are completely finished with this assignment (you should not be updating your model based on test set results!). Before you do that, please modify and include the assert below which confirms that your final dataset has only 10 features (change X to whatever you are calling your training matrix)\n",
    "\n",
    "Quality and efficiency tips:\n",
    "\n",
    "- The most efficient way to do feature ablation is simply to remove a column (check out for instance [np.delete](https://numpy.org/doc/stable/reference/generated/numpy.delete.html). Note that delete doesn't work right with sparse matricies, so you'll definitely want to turn your sparse matrices into ndarrays before doing this. For each feature, you will have to remove it from both the training and dev matrices before building and testing the model.\n",
    "- There are a few natural ways to modularize into functions, and avoid repeated code. For instance, you can have a general function for deleting a feature from any feature matrix, and a function which does the entire ablation process for one provided feature: given feature matrices and one feature, the function would delete the feature from the matrices, train the model on the ablated training matrix, and test the model on the ablated test matrix.\n",
    "- As always document your functions, and add a few tests to show how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[1] <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# otherwise, following classifiers usually show good results\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# !python3 -m pip install lightgbm\n",
    "# it requries cmake: e.g. brew install cmake \n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "# !python3 -m pip  install catboost\n",
    "# OR for M1 (MacBook) (see below)\n",
    "# !wget https://github.com/catboost/catboost/files/7259158/catboost-0.26.1-cp38-none-macosx_11_0_arm64.whl.zip\n",
    "# !unzip catboost-0.26.1-cp38-none-macosx_11_0_arm64.whl.zip\n",
    "# !python3 -m pip install catboost-0.26.1-cp38-none-macosx_11_0_arm64.whl\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Report\n",
    "rubric={raw:2,reasoning:3,viz:1,writing:1}\n",
    "\n",
    "You need to write a short report about your project, namely:\n",
    "\n",
    "- What features did you use, and why did you use them? You need to be able to say something more about each feature than just \"to satisfy the lab requirements\". You can discuss how they relate to your assumptions about differences, what you saw in the texts (if you looked at some), or any statistical analyses you did to help identify features. You will lose points here if there is no rational basis for your selection of features.\n",
    "- How did the features perform? Did they work as you expect? What did you end up dropping? To deepen your discussion, you might want to display the decision tree using [sklearn.tree.plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html).And you should include a simple graph or table showing your results.\n",
    "- Please discuss what each member of your group did. Though it is not necessary that every group member has a equal contribution in terms of code, every group member should have a significant contribution, a major part of the lab for which they were a primary contributor. If any team member fails to contribute significantly, the rest of the group, assuming they have made efforts to encourage the team member to contribute, should discuss what happened in the report, otherwise all team members might lose the raw points for this rubric. Team members who failed to contribute may lose some or all of their grade for the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Optional competition\n",
    "rubric={raw:2}\n",
    "\n",
    "If you like, you may continue to develop your model, including features or better algorithms/hyperparameters, and test your skills against the other teams in the class. The teams with the top 4 scores in the course on the _test_ set will get bonus points as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1st place: 2 bonus\n",
    "2nd place: 1.5 bonus\n",
    "3rd place: 1 bonus\n",
    "4th place: 0.5 bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a tie, the relevant points are split between teams. all the rules for features apply for this competition, as well, you can't have more than 10 features in your final model, so you need to make them good! For your final submission, you should use the model with your best score on the dev set. Any sign that you have developed your model using the test set will invalidate the final score you get and make it impossible to get any bonus points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
